# Explaining Model Predictions with Interpretability Techniques

In this notebook, we will explore various methods to explain the predictions of a Convolutional Neural Network (CNN) trained to classify images from the "Dogs vs. Cats" dataset. Understanding how a model makes predictions is crucial for improving its performance and ensuring transparency.

We will implement and visualize three key explainer techniques:
1. **Integrated Gradients**
2. **Grad-CAM (Gradient-weighted Class Activation Mapping)**

These methods help highlight the most important parts of the input image that contributed to the final decision made by the model.
